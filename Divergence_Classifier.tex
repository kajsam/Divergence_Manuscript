As with all classification problems, if we knew the exact nature of the data, an optimal classifier could be chosen. 
Instead, we have to choose between general assumptions that lead to sub-optimal classifiers, or stronger assumptions that ensure higher performance, but only if the assumptions are correct. 
There are three fundamental levels for MI problems: \\
(1) Choice of MI assumption \\
(2) Estimation \\
(3) Choice of classifier \\
These three steps are present both for deterministic and probabilistic viewpoint, and all steps are closely linked together.
The MI assumption directly implies what to estimate, e.g., $\mathcal{X}^+$ if $\mathcal{X}^+ \cap \mathcal{X}^- = \emptyset$, and also choice of classifier, which in turn is influenced by the estimation method. 

In the probabilistic framework, the choice of MI assumption can be stated as restrictions on the underlying pdfs. 
The pdfs that are used in classification of new bags have to be estimated based on the available bags with labels. 
Finally, the classifier can be chosen and trained based on the previous assumptions and choices. 

The most general MI assumption in the probabilistic framework is that $\tau^+ \neq \tau^-$ (the pdf of positive instances is different from the pdf of negative instances) and $\Pi_{pos}^+ \neq \Pi_{neg}^+$ (the distribution of the proportion of positive instances in a positive bag is different form that of positive instances in a negative bag). 










