A bag consists of several instances. 
An instance is an observation from an underlying distribution. Hence, we are making a {\it collective} assumption.
However, the classification does not simply follow the posterior probability of a bag belonging to a class. 
In the collective assumption, the choices/estimates are
the distribution:\\
\begin{align}
  P(X|\mathbf{\theta}_{b_i}), \, &i = 1, 2, \ldots, n_{bag} \\
  P(X|\mathbf{\theta}_{c_j}), \, & j = 1, 2, \ldots, n_{class}
\end{align}
a divergence: \\
\begin{align}
  & D(P(X|\mathbf{\theta}_{b_i}), P(X|\mathbf{\theta}_{c_j}))
\end{align}
and a classification 
\begin{align}
  \min_{\forall j} \{ D(b_i, c_j)\}
\end{align}

Following the taxonomy of Foulders and Franks and Amores, we have the following: \\
{ \bf Instance-level vs bag-level information:} 
The information is at bag-level. 
Amores also points out the drawbacks of instance-level information. \\
{\bf Implicit vs explicit information:} 
One method is to transform the multiple instances of a bag to a single vector, and the use single-instance learning. 
This could have been used with distribution information by estimating the a parameter vector. 
However, it is not suited in most occasions, because the knowledge about the distribution is too little. \\
{\bf Dissimilarity:} 
This is referred to as {\it distance}, but is not necessarily a distance function in the mathematical definition. 
This is a very influential choice. 
The previously proposed methods use bag-to-bag distance, but we will introduce the use of bag-to-class distance. 

